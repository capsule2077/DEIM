__include__: [
  './rtdetrv2_r50vd_m_7x_coco.yml',
  '../base/rt_deim.yml',]

num_classes: 80
remap_mscoco_category: False

output_dir: ./outputs/rf100vl

RTDETRTransformerv2:
  eval_idx: -1

DEIM:
  backbone: HGNetv2
  encoder: HybridEncoder
  decoder: RTDETRTransformerv2

HGNetv2:
  name: 'B4'
  return_idx: [1, 2, 3]
  freeze_stem_only: True
  freeze_at: 0
  freeze_norm: True


HybridEncoder:
  in_channels: [512, 1024, 2048]
  feat_strides: [8, 16, 32]

  # intra
  hidden_dim: 256
  use_encoder_idx: [2]
  num_encoder_layers: 1
  nhead: 8
  dim_feedforward: 1024
  dropout: 0.
  enc_act: gelu

  # cross
  expansion: 1.0
  depth_mult: 1
  act: silu

optimizer:
  type: AdamW
  params:
    -
      params: '^(?=.*backbone)(?!.*norm|bn).*$'
      lr: 0.00001
    -
      params: '^(?=.*(?:encoder|decoder))(?=.*(?:norm|bn)).*$'
      weight_decay: 0.

  lr: 0.0001
  betas: [0.9, 0.999]
  weight_decay: 0.0001
  
# Increase to search for the optimal ema
epoches: 24 # 72 + 2n

## Our LR-Scheduler
lrsheduler: flatcosine
lr_gamma: 1
warmup_iter: 0    # 0
flat_epoch: 1200    # 4 + epoch // 2, e.g., 40 = 4 + 72 / 2
no_aug_epoch: 4

## Our DataAug
train_dataloader: 
  dataset: 
    type: CocoDetection
    img_folder: /datassd/COCO/train2017/
    ann_file: /datassd/COCO/annotations/instances_train2017.json
    return_masks: False
    transforms:
      policy:
        epoch: [2, 12, 20]   # list 

  collate_fn:
    mixup_epochs: [2, 12]
    stop_epoch: 20
  total_batch_size: 8

val_dataloader:
  type: DataLoader
  dataset: 
    type: CocoDetection
    img_folder: /datassd/COCO/val2017/
    ann_file: /datassd/COCO/annotations/instances_val2017.json
    return_masks: False
  total_batch_size: 8
